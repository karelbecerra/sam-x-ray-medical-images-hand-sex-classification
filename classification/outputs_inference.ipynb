{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.utils import config\n",
    "from src.loaddataset import load_dataset_for_inference\n",
    "from src.buildmodel import build_model\n",
    "from src.utils.modelfile import load_model\n",
    "\n",
    "from stats.confusion_matrix import get_confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "def build_and_load_model(parameters, model_file ):\n",
    "    model, device = build_model(parameters)\n",
    "    load_model(parameters=parameters, model=model, model_file=model_file+'.pth')  \n",
    "    model.eval()\n",
    "    return model, device\n",
    "    \n",
    "def inference(model, device, data_file, batch_size = 600):\n",
    "\n",
    "    test_loader = load_dataset_for_inference(data_file=data_file,  batch_size=batch_size)\n",
    "\n",
    "    # Iterate over batches and evaluate the model\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            x, target = data\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            # Perform inference\n",
    "            #x_data = torch.cat((x_data, x), 0)  if i != 0 else x\n",
    "            #y_data = torch.cat((y_data, target), 0)  if i != 0 else target\n",
    "            #out_data = torch.cat((out_data, out ), 0) if i != 0 else out\n",
    "            x = x.cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            out = out.cpu().numpy()\n",
    "            #x_data = np.concatenate((x_data, x), axis=0) if i != 0 else x\n",
    "            y_data = np.concatenate((y_data, target), axis=0) if i != 0 else target\n",
    "            out_data = np.concatenate((out_data, out), axis=0) if i != 0 else out\n",
    "\n",
    "    return y_data, out_data\n",
    "\n",
    "def get_data_file(dataset, parameters):\n",
    "    match dataset:\n",
    "        case 'train':\n",
    "            return parameters.dataset.train_file\n",
    "        case 'validation':\n",
    "            return parameters.dataset.validation_file\n",
    "        case _:\n",
    "            return parameters.dataset.test_file\n",
    "    \n",
    "def save_output(config_file, dataset, result_data):\n",
    "    folder = 'output/data'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    np.save(os.path.join(folder, config_file + '_' + dataset + '.npy'), result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **** dataset train *****\n",
      " **** run # 1 *****\n",
      "Loading model ... Best epoch (53), Min loss (0.5036070200346284) \n",
      " **** run # 2 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.5027891559115911) \n",
      " **** run # 3 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5068717699939922) \n",
      " **** run # 4 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5057908369828079) \n",
      " **** run # 5 *****\n",
      "Loading model ... Best epoch (49), Min loss (0.5067170019877159) \n",
      " **** run # 6 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.4994514473414017) \n",
      " **** run # 7 *****\n",
      "Loading model ... Best epoch (51), Min loss (0.497221813363544) \n",
      " **** run # 8 *****\n",
      "Loading model ... Best epoch (71), Min loss (0.49993308974524675) \n",
      " **** run # 9 *****\n",
      "Loading model ... Best epoch (35), Min loss (0.5025398569591975) \n",
      " **** run # 10 *****\n",
      "Loading model ... Best epoch (60), Min loss (0.49545190495959784) \n",
      " **** dataset validation *****\n",
      " **** run # 1 *****\n",
      "Loading model ... Best epoch (53), Min loss (0.5036070200346284) \n",
      " **** run # 2 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.5027891559115911) \n",
      " **** run # 3 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5068717699939922) \n",
      " **** run # 4 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5057908369828079) \n",
      " **** run # 5 *****\n",
      "Loading model ... Best epoch (49), Min loss (0.5067170019877159) \n",
      " **** run # 6 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.4994514473414017) \n",
      " **** run # 7 *****\n",
      "Loading model ... Best epoch (51), Min loss (0.497221813363544) \n",
      " **** run # 8 *****\n",
      "Loading model ... Best epoch (71), Min loss (0.49993308974524675) \n",
      " **** run # 9 *****\n",
      "Loading model ... Best epoch (35), Min loss (0.5025398569591975) \n",
      " **** run # 10 *****\n",
      "Loading model ... Best epoch (60), Min loss (0.49545190495959784) \n",
      " **** dataset test *****\n",
      " **** run # 1 *****\n",
      "Loading model ... Best epoch (53), Min loss (0.5036070200346284) \n",
      " **** run # 2 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.5027891559115911) \n",
      " **** run # 3 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5068717699939922) \n",
      " **** run # 4 *****\n",
      "Loading model ... Best epoch (46), Min loss (0.5057908369828079) \n",
      " **** run # 5 *****\n",
      "Loading model ... Best epoch (49), Min loss (0.5067170019877159) \n",
      " **** run # 6 *****\n",
      "Loading model ... Best epoch (61), Min loss (0.4994514473414017) \n",
      " **** run # 7 *****\n",
      "Loading model ... Best epoch (51), Min loss (0.497221813363544) \n",
      " **** run # 8 *****\n",
      "Loading model ... Best epoch (71), Min loss (0.49993308974524675) \n",
      " **** run # 9 *****\n",
      "Loading model ... Best epoch (35), Min loss (0.5025398569591975) \n",
      " **** run # 10 *****\n",
      "Loading model ... Best epoch (60), Min loss (0.49545190495959784) \n"
     ]
    }
   ],
   "source": [
    "runs = 10\n",
    "model_name = 'only_masks'\n",
    "config_file = 'cave'\n",
    "\n",
    "parameters = config.load_parameters(config_file)\n",
    "for dataset in ['train', 'validation', 'test']:\n",
    "  print(' **** dataset ' + dataset + ' *****')\n",
    "  result_data = np.empty((runs,))\n",
    "  for run in range(1, runs + 1):\n",
    "    print(' **** run # ' + str(run) + ' *****')\n",
    "    model, device = build_and_load_model(parameters=parameters, model_file=model_name+ '_run' + str(run))\n",
    "    _, out_data = inference(model=model, device=device, data_file=get_data_file(dataset, parameters))\n",
    "    #print('x_data.shape', x_data.shape)\n",
    "    #print('y_data.shape', y_data.shape)\n",
    "    if run == 1:\n",
    "        result_data = np.empty((runs, out_data.shape[0], out_data.shape[1]))\n",
    "    result_data[run-1, :, :] = out_data\n",
    "    #np.stack((y_data, outputs), axis=2)\n",
    "  save_output(config_file + '_' + model_name, dataset, result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n",
      "(200, 2)\n",
      "[1 1]\n",
      "[0.88018877 0.96000449]\n",
      "1\n",
      "0.880188768136165\n",
      "1\n",
      "0.9600044858828005\n",
      "NEW ====\n",
      "[[1.         0.88018877]\n",
      " [1.         0.96000449]]\n",
      "[1.         0.88018877]\n",
      "[1.         0.96000449]\n",
      "(200, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming y_data and outputs are NumPy arrays with shape (1425, 2)\n",
    "y_data = np.random.choice([0, 1], size=(200, 2))\n",
    "outputs = np.random.rand(200, 2)\n",
    "print(y_data.shape)\n",
    "print(outputs.shape)\n",
    "print(y_data[0])\n",
    "print(outputs[0])\n",
    "print(y_data[0][0])\n",
    "print(outputs[0][0])\n",
    "print(y_data[0][1])\n",
    "print(outputs[0][1])\n",
    "\n",
    "# Stack y_data and outputs along a new axis\n",
    "new_array = np.stack((y_data, outputs), axis=2)\n",
    "print('NEW ====')\n",
    "print(new_array[0])\n",
    "print(new_array[0][0])\n",
    "print(new_array[0][1])\n",
    "print(new_array.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
